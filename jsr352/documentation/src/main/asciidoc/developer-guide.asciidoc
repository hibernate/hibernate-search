[[developer-guide]]
== Developer Guide


=== Batch approach under JSR352

The mass indexer in Google Summer of Code uses the standardized batch processing
architecture JSR 352. Comparing to the current mass indexer in Hibernate Search,
 it provides more clarified work flows and job roles. This clarification helps
 to avoid code confusion and makes each batch component aligned with the Java
 standard batch design. The work flow of this index is described in job xml
 file:

====
[source, SHELL]
$GSOC_HOME/core/src/main/resources/META-INF/batch-jobs/mass-index.xml
====

This job has 3 steps: before-chunk batchlet / chunk / after-chunk batchlet.
Chunk is the core part of the project, the mass indexing. Before-chunk batchlet
contains purge and optimization before the start of indexation and after-chunk
batchlet contains optimization after the end of indexation.


=== Chunk

A basic chunk is composed by 3 components: item reader, item processor and item
writer. One chunk has N items. Chunk reads one item, processes one item, then
stores into an item list. When all the N items have been processed, then item
writer writes them into the destination.

image::jsr352_chunk.png[The chunk sequence diagram of JSR 352]

==== Item reader and item processor

In mass indexer, reading an item means loading an entity using JPA. During the
open of read stream, item reader builds a scrollable result. Then it scrolls one
entity at each reading. The scrolled entity will be processed into an
add-lucene-work by the item processor. This process continues N times until the
checkpoint reaches. Once reached, item writer writes the N items into the
destination and checkpoint will be persisted to the runtime. The number N is
determined by the checkpoint algorithm. This algorithm tells the batch runtime
when the chunk is ready to checkpoint. User can define his own checkpoint
frequency using the mass-indexer interface:

====
[source, JAVA]
massIndexer = massIndexer.checkpointFreq( 1000 );
====

For each read, the checkpoint ID will be updated. When one chunk is finished,
the checkpoint ID will be passed to the batch runtime, which can be reuse in
case of job restart. Each reader has their own session and they’re not shared
with other reader. However, the session for item reader is the same as the
session for item processor, they’re located in the same thread. Item reader has
a boundary, it reads only entities belonging to the target interval, which is
greater or equal to the lower bound and less than the upper bound. A chunk
reader reaches its end when there’s no more result to read.


==== Item writer

Item writer executes the add-lucene-works received from the item processor.


==== Persistence

The new mass indexer interacts with databases using JPA. We need you to
configure a persistence unit and provide an entity manager factory, so that the
new mass indexer can use it.

If you’re under Java EE, there’s no additional configuration before the job
start. Entity manager factory will be injected by annotation
`@PersistenceUnit(name = “massindex”)` for all the internal classes. However,
you need to ensure the existence of persistence unit “massindex”, so that we can
detect it and do the CDI. If you’re under Java SE, please confirm the SE
environment and assign specifically the entity manager factory via massindexer’s
related method.

====
[source, JAVA]
massIndexer.isJavaSE( true )
    .entityManagerFactory( emf );
====

Please make sure that the entity manager factory provided is opened and won’t be
closed during the mass indexing process.


=== Partitioning

In order to maximize the performance, we highly recommend you to speed up the
mass indexer using parallelism. Parallelism is activated by default. Under the
standard JSR 352, the exact word is “partitioning”. The indexing step may run as
multiple partitions, one per thread. Each partition has its own partition ID and
parameters. If there’re more partitions than threads, partitions are considered
as a queue to consume: each thread can only run one partition at a time and
won’t consume the next partition until the previous one is finished.

Mass indexer defines dynamically the number of partitions, but you need to tell
it the number of rows per partitions so that it can calculate the partitions by
division.

====
[source, JAVA]
massIndexer = massIndexer.rowsPerPartition( 5000 );
====

Each partition deals with one root entity type, so 2 different entity types will
never run under the same partition.


==== Partition Mapper

Partition mapper provides a partition plan for chunk partitioning. Each entity
type will receive a different number of partitions according to the number of
entities to index. For each partition, it has its own boundary which is defined
by a scrollable result. Boundaries are defined by dividers. For N partitions
within one entity type, there will be N - 1 dividers. Each divider has 2 roles:
it is the R-limit of the left partition and the L-limit of the right partition.

.A partition example 
[width="60%",frame="topbot",options="header"]
|===============================================
|Partition ID |Entity type |Partition boundaries
|0            |Company     |[N/A, N/A[
|1            |Employee    |[1, 1000[
|2            |Employee    |[1000, 2000[
|3            |Employee    |[2000, 3000[
|4            |Employee    |[3000, 4000[
|5            |Employee    |[4000, N/A[
|===============================================


==== Threads

The maximum number of threads used by the job execution is defined through
method `maxThreads()`. Within the N threads given, there’s 1 thread reserved for
the core, so only N – 1 threads are available for different partitions. The
default number is 10, which correspond to the default max number of threads in
JBeret, the WildFly batch runtime. However, you can overwrite it with your own
value.

====
[source, JAVA]
massIndexer = massIndexer.maxThreads( 5 );
====

Note that the batch runtime cannot guarantee the requested number of threads are
available, it will use as it can up to the requested maximum. (JSR352 v1.0 Final
Release, page 34)


==== Benefits

This approach is aligned with DB caches and hard drive page reading and it only
requires ids to be comparable i.e. they do not need to honor mod nor any other
mathematical function, e.g. mod, hash, which are more likely to be more CPU
intensive to the DB.


==== Inconvenient

- Defining partitions is complicated to understand.
- DB iteration reading is 2 times. 1 iteration for calculating the dividers and
  1 for the Lucene document production (1 / N per partition).
- Giving the partition plan to a large amount of partitions is slow.
- Hashing	Not required for PK with primitive types and String. May be required
  for composite PK.


=== Checkpoint

Mass indexer supports checkpoint algorithm. Assume that N is the value of
checkpoint frequency, then a partition will reach at checkpoint every N items
processed inside the partition. You can overwrite it to adapt your business
requirement. By default, it is set to 3.

====
[source, JAVA]
massIndexer = massIndexer.checkpointFreq( 1000 );
====

Checkpoint a partition-scope algorithm, which means each partition has its own
checkpoint counter and it is not shared with other partitions.


=== Java SE

The major configuration has no difference for Java SE and Java EE. Only 2
additional configurations will be found in Java SE. They’re entity manager
factory and job operator. In Java SE, the entity manager factory should be
manually assigned by the user. The same for job operator.

====
[source, JAVA]
massIndexer.entityManagerFactory( emf );
massIndexer.jobOperator( myJobOperator );
====

In Java SE, the method BatchRuntime#getJobOperator should only be called once,
which ensures that all the classes are using the same operator to coordinate
different job.


==== Test

There’re currently 3 groups of tests in the project, for Java SE, Java EE and
performance. Java SE tests cover the unit tests and integration tests for the
core module. They validate the mass indexer property configuration, the
partition plan, DB access, integrity of indexation and the restartability of the
 job. The Java EE test validate the integrity of indexation and the
 restartability of the job. Performance test compares the performance between
 the existing mass indexer and the new mass indexer.

.Tests location
[width="60%",frame="topbot",options="header"]
|=============================================================
|Test        |Location
|Java SE     |`core`
|Java EE     |`integrationtest/wildfly`
|Performance |`integrationtest/wildfly` (will be changed soon)
|=============================================================


=== TODO

There’re still a lot of things to take care to really make this project
operational. Here’re some ideas:

- Transaction timeout. Currently we don’t have the issue of TX, but face to a
  large dataset, it might cause problem. And I don’t know how to handle it. We
  need to find out how to let people configure their timeout via the mass
  indexer (builder).
- Performance. The performance shows that the new mass indexer is not more
  efficient than the existing one. But I think it can probably become faster.
- The project is not part of the WildFly as a module. This should be done before
  the end.
- The new mass indexer is not able to handle entity types with composite PK.
- JPQL

