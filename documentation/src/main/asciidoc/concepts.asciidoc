= Concepts

[[concepts-full-text]]
== Full-text search

Full-text search is a set of techniques for searching,
in a corpus of text documents,
the documents that best match a given query.

The main difference with traditional search -- for example in an SQL database --
is that the stored text is not considered as a single block of text,
but as a collection of tokens (words).

Hibernate Search relies on either http://lucene.apache.org/[Apache Lucene]
or https://www.elastic.co/products/elasticsearch[Elasticsearch]
to implement full-text search.
Since Elasticsearch uses Lucene internally,
they share a lot of characteristics and their general approach to full-text search.

To simplify, these search engines are based on the concept of inverted indexes:
a dictionary where the key is a token (word) found in a document,
and the value is the list of identifiers of every document containing this token.

Still simplifying, once all documents are indexed,
searching for documents involves three steps:

. extracting tokens (words) from the query;
. looking up these tokens in the index to find matching documents;
. aggregating the results of the lookups to produce a list of matching documents.

[NOTE]
====
Lucene and Elasticsearch are not limited to just text search: numeric data is also supported,
enabling support for integers, doubles, longs, dates, etc.
These types are indexed and queried using a slightly different approach,
which obviously does not involve text processing.
====

[[concepts-mapping]]
== Mapping

Applications targeted by Hibernate search generally use an entity-based model to represent data.
In this model, each entity is a single object with a few properties of atomic type
(`String`, `Integer`, `LocalDate`, ...).
Each entity can have multiple associations to one or even many other entities.

Entities are thus organized as a graph,
where each node is an entity and each association is an edge.

By contrast, Lucene and Elasticsearch work with documents.
Each document is a collection of "fields",
each field being assigned a name -- a unique string --
and a value -- which can be text, but also numeric data such as an integer or a date.
Fields also have a type, which not only determines the type of values (text/numeric),
but more importantly the way this value will be stored: indexed, stored, with doc values, etc.
It is possible to introduce nested documents, but not real associations.

Documents are thus organized, at best, as a collection of trees,
where each tree is a document, optionally with nested documents.

There are multiple mismatches between the entity model and the document model:
properties vs. fields, associations vs. nested documents, graph vs. collection of trees.

The goal of _mapping_, in Hibernate search, is to resolve these mismatches
by defining how to transform one or more entities into a document,
and how to resolve a search hit back into the original entity.
This is the main added value of Hibernate Search,
the basis for everything else from automatic indexing to the various search DSLs.

Mapping is usually configured using annotations in the entity model,
but this can also be achieved using a programmatic API.
To learn more about how to configure mapping, see <<mapper-orm-mapping>>.

To learn how to index the resulting documents, see <<mapper-orm-indexing>>
(hint: it's automatic).

To learn how to search with an API
that takes advantage of the mapping to be closer to the entity model,
in particular by returning hits as entities instead of just document identifiers,
see <<search-dsl>>.

[[concepts-analysis]]
== Analysis
// Search 5 anchors backward compatibility
[[analyzer]]

As mentioned in <<concepts-full-text>>,
the full-text engine works on tokens,
which means text has to be processed
both when indexing (document processing, to build the token -> document index)
and when searching (query processing, to generate a list of tokens to look up).

However, the processing is not *just* about "tokenizing".
Index lookups are *exact* lookups,
which means that looking up `Great` (capitalized) will not return documents containing only `great` (all lowercase).
An extra step is performed when processing text to address this caveat:
token filtering, which normalizes tokens.
Thanks to that "normalization",
`Great` will be indexed as `great`,
so that an index lookup for the query `great` will match as expected.

In the Lucene world (Lucene, Elasticsearch, Solr, ...),
text processing during both the indexing and searching phases
is called "analysis" and is performed by an "analyzer".

The analyzer is made up of three types of components,
which will each process the text successively in the following order:

. Character filter: transforms the input characters. Replaces, adds or removes characters.
. Tokenizer: splits the text into several words, called "tokens".
. Token filter: transforms the tokens. Replaces, add or removes characters in a token,
derives new tokens from the existing ones, removes tokens based on some condition, ...

The tokenizer usually splits on whitespaces (though there are other options).
Token filters are usually where customization takes place.
They can remove accented characters,
remove meaningless suffixes (`-ing`, `-s`, ...)
or tokens (`a`, `the`, ...),
replace tokens with a chosen spelling (`wi-fi` => `wifi`),
etc.

[TIP]
====
Character filters, though useful, are rarely used,
because they have no knowledge of token boundaries.

Unless you know what you are doing,
you should generally favor token filters.
====

In some cases, it is necessary to index text in one block,
without any tokenization:

* For some types of text, such as SKUs or other business codes,
tokenization simply does not make sense: the text is a single "keyword".
* For sorts by field value, tokenization is not necessary.
It is also forbidden in Hibernate Search due to performance issues;
only non-tokenized fields can be sorted on.

To address these use cases,
a special type of analyzer, called "normalizer", is available.
Normalizers are simply analyzers that are guaranteed not to use a tokenizer:
they can only use character filters and token filters.

In Hibernate Search, analyzers and normalizers are referenced by their name,
for example <<mapper-orm-directfieldmapping-analyzer,when defining a full-text field>>.
Analyzers and normalizers have two separate namespaces.

Some names are already assigned to built-in analyzers (in Elasticsearch in particular),
but it is possible (and recommended) to assign names to custom analyzers and normalizers,
assembled using built-in components (tokenizers, filters) to address your specific needs.

Each backend exposes its own APIs to define analyzers and normalizers,
and generally to configure analysis.
See the documentation of each backend for more information:

* <<backend-lucene-analysis,Analysis for the Lucene backend>>
* <<backend-elasticsearch-analysis,Analysis for the Elasticsearch backend>>

[[concepts-sharding-routing]]
== Sharding and routing

Sharding consists in splitting index data into multiple "smaller indexes", called shards,
in order to improve performance when dealing with large amounts of data.

In Hibernate Search, similarly to Elasticsearch,
another concept is closely related to sharding: routing.
Routing consists in resolving a document identifier,
or generally any string called a "routing key",
into the corresponding shard.

When indexing:

* A document identifier and optionally a routing key
are generated from the indexed entity.
* The document, along with its identifier and optionally its routing key,
is passed to the backend.
* The backend "routes" the document to the correct shard.
* The document is indexed in that shard.

When searching:

* The search query can optionally be passed one or more routing keys.
* If no routing key was passed,
the query will be executed on all shards.
* If one or more routing keys were passed,
the backend will resolve these routing key into a set of shards,
and the query will only be executed on all shards,
ignoring the other shards.

Sharding, then, can be leveraged to boost performance in two ways:

* When indexing: a sharded index can spread the "stress"
onto multiple shards, which can be located on different disks (Lucene)
or even different servers (Elasticsearch).
* When searching: if one property, let's call it `category`,
is often used to select a subset of documents,
this property can be <<mapper-orm-bridge-routingkeybridge,defined as a routing key in the mapping>>,
so that it's used to route documents instead of the document ID.
As a result, documents with the same value for `category` will be indexed in the same shard.
Then when searching, if a query already filters documents so that it is known that the hits
will all have the same value for `category`,
the query can be <<search-dsl-query-routing,routed to the shards containing documents with this value>>,
*and the other shards can be ignored*.

To enable sharding, some configuration is required:

* The backends require explicit configuration:
see <<backend-lucene-configuration-sharding,here for Lucene>>
and <<backend-elasticsearch-configuration-sharding,here for Elasticsearch>>.
* In most cases, document IDs are used to route documents to shards by default.
This does not allow taking advantage of routing when searching,
which requires multiple documents to share the same routing key.
To define the routing key to assign to each document,
assign <<mapper-orm-bridge-routingkeybridge,routing key bridges>> to your entities.

[WARNING]
====
Sharding is static by nature:

* Each entity, and its corresponding document,
is expected to stay in the same shard from its creation to its deletion.
Modifying an entity in such a way that its routing key,
and thus its corresponding shard, changes,
will lead to duplicate documents.
Thus, properties used to generate routing keys must be immutable.
* Each index is expected to have the same shards, with the same identifiers,
from one boot to the other.
Changing the number of shards or their identifiers will require full reindexing.
====
