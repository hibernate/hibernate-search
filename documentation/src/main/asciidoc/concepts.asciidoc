= Concepts

[[concepts-full-text]]
== Full-text search

include::todo-placeholder.asciidoc[]

// TODO maybe give a short introduction to full-text search and full-text indexes?

[[concepts-mapping]]
== Mapping

include::todo-placeholder.asciidoc[]

// TODO maybe explain what we mean by "mapping"?
// TODO explain what an "entity" is and what it implies

[[concepts-analysis]]
== Analysis
// Search 5 anchors backward compatibility
[[analyzer]]

Roughly speaking, performing a search query in a full-text search engine involves:

. extracting tokens (words) from the query;
. looking up these words in the index,
which (still roughly speaking) maps each token to the list of documents that contain this token;
. aggregating the results of the lookups to produce a list of matching documents.

This need for "tokens" means that text has to be processed,
both when indexing (document processing, to build the token -> document map)
and when searching (query processing, to generate a list of tokens to look up).

However, the processing is not *just* about "tokenizing".
Index lookups are *exact* lookups,
which means that looking up `Great` (capitalized) will not return documents containing only `great` (all lowercase).
An extra step is performed when processing text to address this caveat:
token filtering, which normalizes tokens.
Thanks to that "normalization",
`Great` will be indexed as `great`,
so that an index lookup for the query `great` will match as expected.

In the Lucene world (Lucene, Elasticsearch, Solr, ...),
text processing during both the indexing and searching phases
is called "analysis" and is performed by an "analyzer".

The analyzer is made up of three types of components,
which will each process the text successively in the following order:

. Character filter: transforms the input characters. Replaces, adds or removes characters.
. Tokenizer: splits the text into several words, called "tokens".
. Token filter: transforms the tokens. Replaces, add or removes characters in a token,
derives new tokens from the existing ones, removes tokens based on some condition, ...

The tokenizer usually splits on whitespaces (though there are other options).
Token filters are usually where customization takes place.
They can remove accented characters,
remove meaningless suffixes (`-ing`, `-s`, ...)
or tokens (`a`, `the`, ...),
replace tokens with a chosen spelling (`wi-fi` => `wifi`),
etc.

[TIP]
====
Character filters, though useful, are rarely used,
because they have no knowledge of token boundaries.

Unless you know what you are doing,
you should generally favor token filters.
====

In some cases, it is necessary to index text in one block,
without any tokenization:

* For some types of text, such as SKUs or other business codes,
tokenization simply does not make sense: the text is a single "keyword".
* For sorts by field value, tokenization is not necessary.
It is also forbidden in Hibernate Search due to performance issues;
only non-tokenized fields can be sorted on.

To address these use cases,
a special type of analyzer, called "normalizer", is available.
Normalizers are simply analyzers that are guaranteed not to use a tokenizer:
they can only use character filters and token filters.

In Hibernate Search, analyzers and normalizers are referenced by their name,
for example <<mapper-orm-directfieldmapping-analyzer,when defining a full-text field>>.
Analyzers and normalizers have two separate namespaces.

Some names are already assigned to built-in analyzers (in Elasticsearch in particular),
but it is possible (and recommended) to assign names to custom analyzers and normalizers,
assembled using built-in components (tokenizers, filters) to address your specific needs.

Each backend exposes its own APIs to define analyzers and normalizers,
and generally to configure analysis.
See the documentation of each backend for more information:

* <<backend-lucene-analysis,Analysis for the Lucene backend>>
* <<backend-elasticsearch-analysis,Analysis for the Elasticsearch backend>>

[[concepts-sharding-routing]]
== Sharding and routing

Sharding consists in splitting index data into multiple "smaller indexes", called shards,
in order to improve performance when dealing with large amounts of data.

In Hibernate Search, similarly to Elasticsearch,
another concept is closely related to sharding: routing.
Routing consists in resolving a document identifier,
or generally any string called a "routing key",
into the corresponding shard.

When indexing:

* A document identifier and optionally a routing key
are generated from the indexed entity.
* The document, along with its identifier and optionally its routing key,
is passed to the backend.
* The backend "routes" the document to the correct shard.
* The document is indexed in that shard.

When searching:

* The search query can optionally be passed one or more routing keys.
* If no routing key was passed,
the query will be executed on all shards.
* If one or more routing keys were passed,
the backend will resolve these routing key into a set of shards,
and the query will only be executed on all shards,
ignoring the other shards.

Sharding, then, can be leveraged to boost performance in two ways:

* When indexing: a sharded index can spread the "stress"
onto multiple shards, which can be located on different disks (Lucene)
or even different servers (Elasticsearch).
* When searching: if one property, let's call it `category`,
is often used to select a subset of documents,
this property can be <<mapper-orm-bridge-routingkeybridge,defined as a routing key in the mapping>>,
so that it's used to route documents instead of the document ID.
As a result, documents with the same value for `category` will be indexed in the same shard.
Then when searching, if a query already filters documents so that it is known that the hits
will all have the same value for `category`,
the query can be <<search-dsl-query-routing,routed to the shards containing documents with this value>>,
*and the other shards can be ignored*.

To enable sharding, some configuration is required:

* The backends require explicit configuration:
see <<backend-lucene-configuration-sharding,here for Lucene>>
and <<backend-elasticsearch-configuration-sharding,here for Elasticsearch>>.
* In most cases, document IDs are used to route documents to shards by default.
This does not allow taking advantage of routing when searching,
which requires multiple documents to share the same routing key.
To define the routing key to assign to each document,
assign <<mapper-orm-bridge-routingkeybridge,routing key bridges>> to your entities.

[WARNING]
====
Sharding is static by nature:

* Each entity, and its corresponding document,
is expected to stay in the same shard from its creation to its deletion.
Modifying an entity in such a way that its routing key,
and thus its corresponding shard, changes,
will lead to duplicate documents.
Thus, properties used to generate routing keys must be immutable.
* Each index is expected to have the same shards, with the same identifiers,
from one boot to the other.
Changing the number of shards or their identifiers will require full reindexing.
====
