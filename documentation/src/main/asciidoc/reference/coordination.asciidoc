[[coordination]]
= Coordination

[[coordination-basics]]
== Basics

include::components/coordination-comparison-note.asciidoc[]

An application using Hibernate Search usually relies on multiple threads,
or even multiple application instances,
which will update the database concurrently.

The coordination strategy defines how these threads/nodes will coordinate with each other
in order to update indexes according to these database updates,
in a way that ensures consistency,
prevents data loss,
and optimizes performance.

This strategy is set through configuration properties:

[source]
----
hibernate.search.coordination.strategy = outbox-polling
----

The default for this property is `none`.

See the following subsections for details about available strategies.

[[coordination-none]]
== No coordination

[[coordination-none-basics]]
=== Basics

The default strategy is the simplest and does not involve any additional infrastructure
for communication between application nodes.

All <<mapper-orm-indexing-automatic,automatic indexing>> operations
are executed directly in application threads,
which gives this strategy the unique ability to provide <<mapper-orm-indexing-automatic-synchronization,synchronous indexing>>,
at the cost of a few limitations:

* <<limitations-parallel-embedded-update>>
* <<limitations-backend-indexing-error>>

include::components/coordination-comparison-note.asciidoc[]

[[coordination-none-indexing]]
=== How indexing works without coordination

[[coordination-none-indexing-detected-changes]]
Changes have to occur in the ORM session in order to be detected::
See <<mapper-orm-indexing-automatic-concepts-changes-in-session>> for more details.
[[coordination-none-indexing-association-consistency]]
Associations must be updated on both sides::
See <<limitations-changes-asymmetric-association-updates>> for more details.
[[coordination-none-indexing-change-filter]]
Only relevant changes trigger indexing::
See <<mapper-orm-indexing-automatic-concepts-dirty-checking>> for more details.
[[coordination-none-indexing-on-flush]]
Entity data is extracted from entities upon session flushes::
[[mapper-orm-indexing-automatic-concepts-extraction-on-flush]] When a Hibernate ORM session is flushed,
Hibernate Search will extract data from the entities
to build documents to index,
and will put these documents in an internal buffer for
<<coordination-none-indexing-guarantee,indexing after the commit>>.
This extraction <<coordination-none-indexing-lazy-loading,may involve loading extra data from the database>>.
+
[NOTE]
====
The fact that this internal buffer is populated on session flush
means that you can safely `clear()` the session after a `flush()`:
entity changes performed up to the flush will be indexed correctly.

If you come from Hibernate Search 5 or earlier,
you may see this as a significant improvement:
there is no need to call `flushToIndexes()` and update indexes in the middle of a transaction anymore,
except for larger volumes of data (see <<mapper-orm-indexing-manual-indexingplan-process-execute>>).
====
+
However, if you perform a batch process inside a transaction,
and call `session.flush()`/`session.clear()` regularly to save memory,
be aware that Hibernate Search's internal buffer holding documents to index
will grow on each flush, and will not be cleared until the transaction is committed or rolled back.
If you encounter memory issues because of that,
see <<mapper-orm-indexing-manual-indexingplan-process-execute>> for a few solutions.
[[coordination-none-indexing-lazy-loading]]
Extraction of entity data may fetch extra data from the database::
[[mapper-orm-indexing-automatic-concepts-indexing-triggers-lazy-loading]] Even when you change only a single property of an indexed entity,
if that property is indexed,
Hibernate Search needs to rebuild the corresponding document *in full*.
+
Hibernate Search tries to only load what is necessary for indexing,
but depending on your mapping, this may lead to lazy associations being loaded just to reindex entities,
even if you didn't need them in your business code,
which may represent an overhead for your application threads
as well as your database.
+
This extra cost can be mitigated to some extent by:
+
* leveraging Hibernate ORM's batch fetching:
see link:{hibernateDocUrl}#configurations-database-fetch[the `batch_fetch_size` property]
and link:{hibernateDocUrl}#fetching-batch[the `@BatchSize` annotation].
* leveraging Hibernate ORM's link:{hibernateDocUrl}#caching[second-level cache],
especially for immutable entities referenced from indexed entities
(e.g. for reference data such as countries, cities, ...).
[[coordination-none-indexing-guarantee]]
Indexing is not guaranteed on commit, but only when the application thread returns::
[[mapper-orm-indexing-automatic-concepts-transaction-commit]] When entity changes happen inside a transaction,
indexes are not updated immediately, but only after the transaction is successfully committed.
That way, if a transaction is rolled back, the indexes will be left in a state consistent with the database,
discarding all the index changes that were planned during the transaction.
+
However, if an error occurs in the backend while indexing,
this behavior means that <<limitations-backend-indexing-error,index changes may be lost, leading to out-of-sync indexes>>.
If this is a problem for you, you should consider switching to <<coordination-outbox-polling,another coordination strategy>>.
+
[[mapper-orm-indexing-automatic-concepts-notransaction-flush]]
NOTE: When entity changes happen outside of any transaction (not recommended),
indexes are updated immediately upon session `flush()`.
Without that flush, indexes will not be updated automatically.
[[coordination-none-indexing-visibility]]
Index changes may not be visible immediately::
[[mapper-orm-indexing-automatic-concepts-nearrealtime]] By default, indexing will resume the application thread after index changes are committed to the indexes.
This means index changes are safely stored to disk,
but this does not mean a search query ran immediately after indexing will take the changes into account:
when using the Elasticsearch backend in particular, changes may take some time to be visible from search queries.
+
See <<mapper-orm-indexing-automatic-synchronization>> for details.

[[coordination-outbox-polling]]
== [[coordination-database-polling]] `outbox-polling`: additional event tables and polling in background processors

include::components/incubating-warning.asciidoc[]

[[coordination-outbox-polling-basics]]
=== [[coordination-database-polling-basics]] Basics

The `outbox-polling` strategy implements coordination through
<<coordination-outbox-polling-schema,additional tables>> in the application database.

<<mapper-orm-indexing-automatic,Automatic indexing>> is implemented
by pushing events to an outbox table within the same transaction as the entity changes,
and polling this outbox table from background processors which perform indexing.

This strategy is able to provide guarantees that entities will be indexed regardless
of temporary I/O errors in backend,
at the cost of being only able to perform this indexing asynchronously.

The `outbox-polling` strategy can be enabled with the following settings:

[source]
----
hibernate.search.coordination.strategy = outbox-polling
----

[CAUTION]
====
If link:{hibernateDocUrl}#configurations-multi-tenancy[multi-tenancy] is enabled,
you will need extra configuration.

See <<coordination-outbox-polling-multi-tenancy>>.
====


You will also need to add this dependency:

[source, XML, subs="+attributes"]
----
<dependency>
   <groupId>org.hibernate.search</groupId>
   <artifactId>hibernate-search-mapper-orm-coordination-outbox-polling</artifactId>
   <version>{hibernateSearchVersion}</version>
</dependency>
----

include::components/coordination-comparison-note.asciidoc[]

[[coordination-outbox-polling-indexing]]
=== [[coordination-database-polling-indexing]] How indexing works with `outbox-polling` coordination

[[coordination-outbox-polling-indexing-detected-changes]]
Changes have to occur in the ORM session in order to be detected::
See <<mapper-orm-indexing-automatic-concepts-changes-in-session>> for more details.
[[coordination-outbox-polling-indexing-association-consistency]]
Associations must be updated on both sides::
See <<limitations-changes-asymmetric-association-updates>> for more details.
[[coordination-outbox-polling-indexing-change-filter]]
Only relevant changes trigger indexing::
See <<mapper-orm-indexing-automatic-concepts-dirty-checking>> for more details.
[[coordination-outbox-polling-indexing-background]]
Indexing happens in a background thread::
When a Hibernate ORM session is flushed,
Hibernate Search will persist entity change events within the same Hibernate ORM session and the same transaction.
+
A <<coordination-outbox-polling-agents-event-processor,event processor>> polls the database for new entity change events,
and asynchronously performs reindexing of the appropriate entities when it finds new events
(i.e. after the transaction is committed).
+
[NOTE]
====
The fact that events are persisted on session flush
means that you can safely `clear()` the session after a `flush()`:
entity changes events detected up to the flush will be persisted correctly.

If you come from Hibernate Search 5 or earlier,
you may see this as a significant improvement:
there is no need to call `flushToIndexes()` and update indexes in the middle of a transaction anymore.
====
[[coordination-outbox-polling-indexing-full-loading]]
The background processor will completely reload entities from the database::
The background processor responsible for reindexing entities
does not have access to the state of the link:{hibernateDocUrl}#best-practices-caching[first level cache]
when the entity change occurred, because it occurred in a different session.
+
This means each time an entity changes and has to be reindexed,
the background process will load that entity in full.
Depending on your mapping, it may also need to load lazy associations to other entities.
+
This extra cost can be mitigated to some extent by:
+
* leveraging Hibernate ORM's batch fetching;
see link:{hibernateDocUrl}#configurations-database-fetch[the `batch_fetch_size` property]
and link:{hibernateDocUrl}#fetching-batch[the `@BatchSize` annotation].
* leveraging Hibernate ORM's link:{hibernateDocUrl}#caching[second-level cache],
especially for immutable entities referenced from indexed entities
(e.g. for reference data such as countries, cities, ...).
[[coordination-outbox-polling-indexing-guarantee]]
Indexing is guaranteed on transaction commit::
When entity changes happen inside a transaction,
Hibernate Search will persist entity change events within the same transaction.
+
If the transaction is committed, these events will be committed as well;
if it rolls back, the events will be rolled back as well.
This guarantees the events will eventually be processed by a background thread
and that the indexes will be updated accordingly,
but only when (if) the transaction succeeds.
+
[[coordination-outbox-polling-indexing-guarantee-notransaction]]
NOTE: When entity changes happen outside of any transaction (not recommended),
events indexes are sent immediately after the session `flush()`.
Without that flush, indexes will not be updated automatically.
[[coordination-outbox-polling-indexing-visibility]]
Index changes will not be visible immediately::
By default, the application thread will resume after entity change events are committed to the database.
This means these changes are safely stored to disk,
but this does not mean a search query ran immediately when the thread resumes will take the changes into account:
<<coordination-outbox-polling-indexing-background,indexing will happen at a later time, asynchronously, in a background processor>>.
+
You can <<coordination-outbox-polling-agents-event-processor,configure this event processor>> to run more often,
but it will remain asynchronous.

[[coordination-outbox-polling-schema]]
=== [[coordination-database-polling-schema]] Impact on the database schema

The `outbox-polling` coordination strategy needs to store data in additional tables in the application database,
so that this data can be consumed by background threads.

This includes in particular an outbox table, to which one row (representing a change event) is pushed every time an entity is changed
in a way that requires reindexing.

This also includes an agent table, where Hibernate Search registers every background event processor
in order to <<coordination-outbox-polling-sharding,dynamically assign shards>> to each application instance,
or simply to check that <<coordination-outbox-polling-sharding-static,statically assigned shards>>
are consistent.

These tables are accessed through entities that are automatically added to the Hibernate ORM configuration,
and as such they should be automatically generated when relying on Hibernate ORM's
link:{hibernateDocUrl}#configurations-hbmddl[automatic schema generation].

If you need to integrate the creation/dropping of these tables to your own script,
the easiest solution is to have Hibernate ORM generate DDL scripts for your whole schema
and copy everything related to constructs (tables, sequences, ...) prefixed with `HSEARCH_`.
See link:{hibernateDocUrl}#configurations-hbmddl[automatic schema generation],
in particular the Hibernate ORM properties `javax.persistence.schema-generation.scripts.action`,
`javax.persistence.schema-generation.scripts.create-target`
and `javax.persistence.schema-generation.scripts.drop-target`.

[[coordination-outbox-polling-sharding]]
=== [[coordination-database-polling-sharding]] [[coordination-outbox-polling-sharding-basics]] Sharding and pulse

In order to avoid unnecessarily indexing the same entity multiple times on different application nodes,
Hibernate Search partitions the entities in what it calls "shards":

* Each entity belongs to exactly one shard.
* Each application node involved in <<coordination-outbox-polling-agents-event-processor,event processing>>
is uniquely assigned one or more shards, and will only process events related to entities in these shards.

In order to reliably assign shards,
Hibernate Search <<coordination-outbox-polling-schema,adds an agent table to the database>>,
and uses that table to register agents involved in indexing (most of the time, one application instance = one agent =
the <<coordination-outbox-polling-agents-event-processor,event processor>>).
The registered agents form a cluster.

To make sure that agents are always assigned one shard,
and that one shard is never assigned to more than one agent,
each agent will periodically perform a "pulse",
updating and inspecting the agent table.

What happens during a pulse depends on the <<coordination-outbox-polling-agents,type of agent>>.
During a "pulse":

* An event processor will:
** update its own entry in the agent table, to let other agents knows it's still active;
** forcibly remove entries from other agents if it detects that these agents expired
   (did not update their entry for a long time);
** detect and report configuration mistakes
   if using <<coordination-outbox-polling-sharding-static,static sharding>>,
   e.g. two agents assigned to the same shard;
** decide to suspend itself if a <<coordination-outbox-polling-agents-mass-indexer,mass indexer>> is running;
** trigger rebalancing as necessary if using <<coordination-outbox-polling-agents-event-processor-sharding,dynamic sharding>>;
   e.g. when it detects that new agents recently joined the cluster,
   or that agents left the cluster (voluntarily or forcibly).
* A mass indexer will:
** update its own entry in the agent table, to let other agents knows it's still active;
** forcibly remove entries from other agents if it detects that these agents expired
   (did not update their entry for a long time);
** switch to active waiting mode (frequent polling) if it notices some
   <<coordination-outbox-polling-agents-event-processor,event processors>> are still running;
** switch to pulse-only mode (infrequent polling) and give the green light for mass indexing to start
  if it notices no <<coordination-outbox-polling-agents-event-processor,event processors>> are running anymore.

For more details about dynamic and static sharding, see the following sections.

[[coordination-outbox-polling-agents]]
=== [[coordination-database-polling-processors]] Agents

The `outbox-polling` coordination strategy involves agents executing in the background.

The following subsections list those agents and detail their respective configuration options.

[[coordination-outbox-polling-agents-event-processor]]
==== Event processor

[[coordination-outbox-polling-agents-event-processor-basics]]
===== Basics

The most important agent is the event processor,
which polls the outbox table for events
and then reindexes the corresponding entities when new events are found.

The event processor can be configured using the following configuration properties:

[source]
----
hibernate.search.coordination.event_processor.enabled = true
hibernate.search.coordination.event_processor.polling_interval = 100
hibernate.search.coordination.event_processor.pulse_interval = 2000
hibernate.search.coordination.event_processor.pulse_expiration = 30000
hibernate.search.coordination.event_processor.batch_size = 50
hibernate.search.coordination.event_processor.transaction_timeout = 10
hibernate.search.coordination.event_processor.retry_delay = 15
----

* `event_processor.enabled` defines whether the event processor is enabled,
as a <<configuration-property-types,boolean value>>.
The default for this property is `true`, but it can be set to `false` to disable event processing on some application nodes,
for example to dedicate some nodes to HTTP request processing and other nodes to event processing.
* `event_processor.polling_interval` defines how long to wait for another query to the outbox events table
after a query didn't return any event,
as an <<configuration-property-types,integer value>> in milliseconds.
The default for this property is `100`.
+
Lower values will reduce the time it takes for a change to be reflected in the index,
but will increase the stress on the database when there are no new events.
* `event_processor.pulse_interval` defines how long the event processor can poll for events
before it must perform a "pulse",
as an <<configuration-property-types,integer value>> in milliseconds.
The default for this property is `2000`.
+
See <<coordination-outbox-polling-sharding-basics,the sharding basics>> for information about "pulses".
+
The pulse interval must be set to a value between the polling interval (see above) and one third (1/3) of the expiration interval (see below).
+
Low values (closer to the polling interval) mean a shorter delay before rebalancing
when a node joins or leaves the cluster,
and reduced risk of incorrectly considering an agent disconnected,
but more stress on the database because of more frequent checks of the list of agents.
+
High values (closer to the expiration interval) mean a longer delay before rebalancing
when a node joins or leaves the cluster,
and increased risk of incorrectly considering an agent disconnected,
but less stress on the database because of less frequent checks of the list of agents.
* `event_processor.pulse_expiration` defines how long an event processor "pulse" remains valid
before considering the processor disconnected and forcibly removing it from the cluster,
as an <<configuration-property-types,integer value>> in milliseconds.
The default for this property is `30000`.
+
See <<coordination-outbox-polling-sharding-basics,the sharding basics>> for information about "pulses".
+
The expiration interval must be set to a value 3 times larger than the pulse interval (see above).
+
Low values (closer to the pulse interval) mean a shorter delay before rebalancing
when a node abruptly leaves the cluster due to a crash or network failure,
but increased risk of incorrectly considering an event processor disconnected.
+
High values (much larger than the pulse interval) mean a longer delay before rebalancing
when a node abruptly leaves the cluster due to a crash or network failure,
but reduced risk of incorrectly considering an event processor disconnected.
* `event_processor.batch_size` defines how many outbox events, at most, are processed in a single transaction
as an <<configuration-property-types,integer value>>.
The default for this property is `50`.
+
Higher values will reduce the number of transactions opened by the background process
and may increase performance thanks to the first-level cache (persistence context),
but will increase memory usage and in extreme cases may lead to `OutOfMemoryErrors`.
* `event_processor.transaction_timeout` defines the timeout for transactions processing outbox events
as an <<configuration-property-types,integer value>> in seconds.
Only effective when a JTA transaction manager is configured.
When using JTA and this property is not set, Hibernate Search will use whatever default transaction timeout is configured in the JTA transaction manager.
* `event_processor.retry_delay` defines the time after which it is possible to process again an event that has gone into error.
as a <<configuration-property-types,positive integer value>> in seconds.
Use the value `0` to reprocess the failed events as soon as possible, with no delay.
The default for this property is `30`.
Dynamic sharding does not require any specific configuration,
but you may want to configure other properties of <<coordination-outbox-polling-agents-event-processor,the event processor>>.

[[coordination-outbox-polling-agents-event-processor-sharding]]
===== [[coordination-database-polling-sharding-dynamic]] [[coordination-database-polling-sharding-static]] Sharding

By default, <<coordination-outbox-polling-sharding,sharding>> is dynamic:
Hibernate Search registers each application instance in the database,
and uses that information to dynamically assign a single, unique shard to each application instance,
updating assignments as instances start or stop.
Dynamic sharding does not accept any configuration beyond the
<<coordination-outbox-polling-agents-event-processor-basics,basics>>.

If you want to configure sharding explicitly, you can use static sharding
by setting the following configuration properties:

[source]
----
hibernate.search.coordination.event_processor.shards.static = true
hibernate.search.coordination.event_processor.shards.total_count = 4
hibernate.search.coordination.event_processor.shards.assigned = 0
----

* `shards.static` defines sharding as static.
It defaults to `false` and must be set to `true` explicitly in order for
the other configuration properties to be taken into account.
* `shards.total_count` defines the total number of shards
as an <<configuration-property-types,integer value>>.
This property has no default and must be set explicitly when static sharding is enabled.
It must be set to the same value on all application nodes with assigned shards.
* `shards.assigned` defines the shards assigned to the application node
as an <<configuration-property-types,integer value>>, or multiple comma-separated integer values.
This property has no default and must be set explicitly when static sharding is enabled.
Shards are referred to by an index in the range `[0, total_count - 1]` (see above for `total_count`).
A given application node must be assigned at least one shard
but may be assigned multiple shards by setting `shards.assigned` to a comma-separated list,
e.g. `0,3`.

[CAUTION]
====
Each shard must be assigned to one and only one application node,
but this is **not** checked automatically by Hibernate Search.

Failure to do so may result in poor performance,
or event out-of-sync indexes.
====

[TIP]
====
Sharding settings are irrelevant, and thus ignored,
when <<coordination-outbox-polling-agents-event-processor,event processing is disabled>>.
====

.Example of static sharding settings
====
For example, the following configuration with 4 application nodes
would assign shard `0` to application node #0, shard `1` application node #1,
and no shard at all to application nodes #2 and #3:

[source]
----
# Node #0
hibernate.search.coordination.strategy = outbox-polling
hibernate.search.coordination.event_processor.shards.static = true
hibernate.search.coordination.event_processor.shards.total_count = 2
hibernate.search.coordination.event_processor.shards.assigned = 0
----

[source]
----
# Node #1
hibernate.search.coordination.strategy = outbox-polling
hibernate.search.coordination.event_processor.shards.static = true
hibernate.search.coordination.event_processor.shards.total_count = 2
hibernate.search.coordination.event_processor.shards.assigned = 1
----

[source]
----
# Node #2
hibernate.search.coordination.strategy = outbox-polling
hibernate.search.coordination.event_processor.enabled = false
----

[source]
----
# Node #3
hibernate.search.coordination.strategy = outbox-polling
hibernate.search.coordination.event_processor.enabled = false
----
====

[TIP]
====
Sharding settings, even static, may change over the lifetime of an application,
but must be consistent across all application nodes involved in processing at all times.

If you cannot stop your whole application cluster,
the only way to safely change sharding settings is to restart nodes one by one
to disable <<coordination-outbox-polling-agents-event-processor,event processing>>,
then (once every node has processing disabled),
restart nodes one by one with the new sharding settings and with processing re-enabled.
====

[[coordination-outbox-polling-agents-mass-indexer]]
==== Mass indexer

[[coordination-outbox-polling-agents-mass-indexer-basics]]
===== Basics

During <<mapper-orm-indexing-massindexer,mass indexing>>,
an application instance will exceptionally bypass <<coordination-database-polling-sharding,sharding>>
and index entities from any shard.

Bypassing sharding can be dangerous,
because indexing the same entity simultaneously from an event processor and the mass indexer
could potentially result in an out-of-sync index in some rare situations.
This is why, to be perfectly safe, event processing gets suspended while mass indexing is in progress.
Events are still produced and persisted, but their processing gets delayed until mass indexing finishes.

The suspension of event processing is achieved by registering a mass indexer agent in the agent table,
which event processors will eventually detect and react to by suspending themselves.
When mass indexing finishes, the mass indexer agent is removed from the agent table,
event processors detect that and resume event processing.

The mass indexer agent can be configured using the following configuration properties:

[source]
----
hibernate.search.coordination.mass_indexer.polling_interval = 100
hibernate.search.coordination.mass_indexer.pulse_interval = 2000
hibernate.search.coordination.mass_indexer.pulse_expiration = 30000
----

* `mass_indexer.polling_interval` defines how long to wait for another query to the agent table
when actively waiting for event processors to suspend themselves,
as an <<configuration-property-types,integer value>> in milliseconds.
The default for this property is `100`.
+
Low values will reduce the time it takes for the mass indexer agent to detect that event processors finally suspended themselves,
but will increase the stress on the database while the mass indexer agent is actively waiting.
+
High values will increase the time it takes for the mass indexer agent to detect that event processors finally suspended themselves,
but will reduce the stress on the database while the mass indexer agent is actively waiting.
* `mass_indexer.pulse_interval` defines how long the mass indexer can wait before it must perform a "pulse",
as an <<configuration-property-types,integer value>> in milliseconds.
The default for this property is `2000`.
+
See <<coordination-outbox-polling-sharding-basics,the sharding basics>> for information about "pulses".
+
The pulse interval must be set to a value between the polling interval (see above) and one third (1/3) of the expiration interval (see below).
+
Low values (closer to the polling interval) mean reduced risk of incorrectly considering a mass indexer agent disconnected,
but more stress on the database because of more frequent updates of the mass indexer agent's entry in the agent table.
+
High values (closer to the expiration interval) mean increased risk of incorrectly considering a mass indexer agent disconnected,
but less stress on the database because of less frequent updates of the mass indexer agent's entry in the agent table.
* `mass_indexer.pulse_expiration` defines how long an event processor "pulse" remains valid
before considering the processor disconnected and forcibly removing it from the cluster,
as an <<configuration-property-types,integer value>> in milliseconds.
The default for this property is `30000`.
+
See <<coordination-outbox-polling-sharding-basics,the sharding basics>> for information about "pulses".
+
The expiration interval must be set to a value 3 times larger than the pulse interval (see above).
+
Low values (closer to the pulse interval) mean a shorter delay before resuming
event processing when a node currently performing mass indexing
abruptly leaves the cluster due to a crash or network failure,
but increased risk of incorrectly considering a mass indexer disconnected.
+
High values (much larger than the pulse interval) mean a longer delay before resuming
event processing when a node currently performing mass indexing
abruptly leaves the cluster due to a crash or network failure,
but reduced risk of incorrectly considering a mass indexer disconnected.

[[coordination-outbox-polling-multi-tenancy]]
=== Multi-tenancy

If you use link:{hibernateDocUrl}#configurations-multi-tenancy[Hibernate ORM's multi-tenancy support],
you will need to <<configuration-multi-tenancy,configure the list of all possible tenant identifiers>>.

Failing to mention a tenant identifier in this configuration might result in
events piling up in the outbox table without ever being <<coordination-outbox-polling-agents-event-processor,processed>>,
or in exceptions being thrown upon <<coordination-outbox-polling-agents-mass-indexer,mass indexing>>
due to incomplete configuration.

Apart from that, multi-tenancy support should be fairly transparent:
Hibernate Search will simply duplicate event processors for each tenant identifiers.

You can use different configuration for different tenants by using a different root for configuration properties:

* `hibernate.search.coordination` is the default root, whose properties will be used as a default for all tenants.
* `hibernate.search.coordination.tenants.<tenant ID>` is the tenant-specific root.

See below for an example.

.Configuration of coordination in a multi-tenant application with a node dedicated to a single tenant
====
Node 1:

[source]
----
hibernate.search.multi_tenancy.tenant_ids=tenant1,tenant2,tenant3,tenant4
hibernate.search.coordination.strategy = outbox-polling
hibernate.search.coordination.tenants.tenant1.event_processor.enabled = false <1>
----
<1> This node will process events for all tenants *except* `tenant1`.

Node 2:

[source]
----
hibernate.search.multi_tenancy.tenant_ids=tenant1,tenant2,tenant3,tenant4
hibernate.search.coordination.strategy = outbox-polling
hibernate.search.coordination.event_processor.enabled = false <1>
hibernate.search.coordination.tenants.tenant1.event_processor.enabled = true <2>
----
<1> This node will not process events for any tenant...
<2> ... *except* `tenant1`.
====
